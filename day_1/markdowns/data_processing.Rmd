---
title: "Tutorial 1: Data processing - from .fastq to .bam"
output: 
  github_document:
    toc:  true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>



In this tutorial, you will learn how to go from raw sequencing files in `fastq` format through alignment files in `bam` format that we can use for downstream analysis. Along the way, you will perform multiple quality control (QC) procedures, and will map the short sequences to a snippet of a reference genome. 

<br>


## Case study for practicals

Throughout this course, you will be working with data from the Atlantic silverside, *Menidia menidia*, a small estuarine fish.


![](../img/IMG_8658_SnyderCredit.jpg)

MORE TO ADD


Add sample table

<br>


### Today's data
Today, we will work with subsets of two different fastq files from each of three Atlantic silversides from the Therkildsen et al. 2019 and Wilder et al. 2020 papers. The libraries were prepared as described in Therkildsen and Palumbi 2017 and sequenced with 125bp paired-end reads on an Illumina HiSeq instrument. There are two different fastq files because each of these individuals were sequenced in two different sequencing runs, to even out sequence coverage among individuals (as discussed in lecture). 

We will map these raw sequence files to the Atlantic silverside genome (Tigano et al. nearly submitted!). To minimize computational time, we are just looking at a small 2 Mb snippet of chromosome 24 for all the exercises in this course.

<br>


## Initial preparation

#### 1. Make sure you're up to speed on basic shell scripting

We'll be working almost exclusively through the command line, so if you have not used shell scripting before or are getting rusty on it, it may be helpful to have a look at a tutorial like [this one](https://linuxconfig.org/bash-scripting-tutorial-for-beginners) or a cheat sheet like [this one](https://bioinformaticsworkbook.org/Appendix/Unix/UnixCheatSheet.html#gsc.tab=0) before proceeding to the next step. 


<br>

#### 2. Copy the working directories with the needed input files

Let's first get set up and retrieve a copy the data we will be working on. Copy the `day_1` directory from `xxx_path`. This directory will be referred to as `BASEDIR` in many of the scripts below. Have a look at your new `day_1` directory; it contains the following subdirectories:
  
  * `raw_fastq` has the raw fastq files we'll be working on today
  
  * `adapter_clipped` is empty, but you'll use it for storing your adapter clipped fastq files
  
  * `bam` is empty, but you'll use it for storing your bam (alignment) files
  
  * `sample_lists` is for storing sample tables, sample lists, and other small text files
  
  * `fastqc` is empty, but you'll use it for storing your FastQC output
  
  * `markdowns` contains this exercise, and you can also add your own markdown notes here
  
  * `reference` currently contains the reference genome file and a list of adapter sequences
  
  * `scripts` is for storing scripts

<br>

> Hint: We move between directories using the `cd` command in the Unix shell. New directories can be created using the `mkdir` command in Unix shell.

<br>


#### 3. Make sure you're familiar with `for loops` in bash

In low-coverage whole genome sequencing datasets, we'll typically have data from hundreds of individuals, so we need an efficient way to process all of these files without having to write a separate line of code for each file. `for loops` are a powerful way to achieve this, and we will be using them in every step of our pipeline, so let's first take a moment to make sure we understand the syntax.


Variables

FOR LOOPS

<br>

#### 4. Orient yourself to the formatting of our sample table and sample list

When we get data files back from the sequencing center, the files often have obscure names, so we need a data table that let's us link those file names to our sample IDs and other information. Often, part of the file name will be identical and part of it will reflect some kind of unique sample identifier (either a name you supplied or a name given by the sequencing center). As an example, look in the `day_1/raw_fastq` folder and notice how all the files end in either `_1.fastq.gz` or `_2.fastq.gz` (these are the forward and reverse sequences) while the first part differs. We call the sample identifier the `prefix`.

Our pipeline is set up to link up these `prefix` names with sample details based on a sample table set up as the example you can find in `day_1/sample_lists/sample_table.tsv`.

For our scripts below to work, the sample table has to be a **tab deliminated** table with the following six columns, strictly in this order:

  * `prefix` the prefix of raw fastq file names
  
  * `lane_number` lane number; each sequencing lane or batch should be assigned a different number
  
  * `seq_id` sequence ID; this variable is only relevant when different libraries were prepared out of the same sample and were run in the same lane (e.g. if you wanted to include a replicate). In this case, seq_id should be used to distinguish these separate libraries.

  * `sample_id` sample ID
  
  * `population` population name
  
  * `data_type` data type; there can only be two possible entries: `pe` (for paired-end data) or `se` (for single end data). We need this in the table because for some of our processing steps, the commands are slightly different for paired-end and single-end data.

It is important to make sure that the combination of lane_number, seq_id, and sample_id is unique for each fastq file. 

<br>

A second file that we'll use is called a sample list. This is simply a list of prefixes for the samples we want to analyze. Our sample table can contain data for all individuals in our study, but at any given time, we may only want to perform an operation on a subset of them. Like today, in the interest of time, we only want to run 6 sets of fastq files through each processing step. 

Have a look at the list we'll be using in `day_1/sample_lists/sample_list.txt` and note that it's just a list of fastq name prefixes, each on a separate line and there should be no header in this file.

<br>

**Activity:**: Compare the `sample_list.txt` to the `sample_table.txt`. Which samples will we be analyzing today?

<br>

With our small sample table and sample list here, we can easily look this up manually. But if we have hundeds of samples, that becomes more cumbersome. Let's automate it with our first `for loop`.

<br>

#### 5. Practice using bash `for loops` to iterate over target samples
For each prefix in our `sample_list.txt` will use `grep` to extract the relevant line from the sample table, use `cut` to extract the column with sample ID, and then `echo` to print the sample ID 

```{bash, eval = FALSE}

BASEDIR=/workdir/physalia-lcwgs/day_1/ # Path to the base directory / project directory.

SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files. It should be a subset of the the 1st column of the sample table.

SAMPLETABLE=$BASEDIR/sample_lists/sample_table.tsv # Path to a sample table where the 1st column is the prefix of the raw fastq files. The 4th column is the sample ID. 


for SAMPLEFILE in `cat $SAMPLELIST`; do   # Loop through each of the prefixes listed in our sample list
	
	# For each prefix, extract the associated sample ID (column 4) from the table
	SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4` 

	echo $SAMPLEFILE refers to sample $SAMPLE_ID
	
done

```

<br>

Now change the `for loop` so it also outputs which population each fastq file has data for.

<details>
  <summary>Click here to see a solution</summary>
  
```{bash, eval = FALSE}

for SAMPLEFILE in `cat $SAMPLELIST`; do   # Loop through each of the prefixes listed in our sample list
	
	# For each prefix, extract the associated sample ID (column 4) and population ID (column 5) from the table
	SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4` 
	POPULATION=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 5` 

	echo $SAMPLEFILE refers to sample $SAMPLE_ID from $POPULATION
	
done

```

</details>


<br>

#### 6. Define paths to the project directory and programs

We need to make sure the server knows where to find the programs we'll be running and our input and output directories. This will always need to be specified every time we run our scripts in a new login session.

<br>

##### Write the project directory as a variable named `BASEDIR`

> Hint: Change the `/workdir/physalia-lcwgs/day_1/` part in the following line to the path of your base directory

```{bash eval=F}

BASEDIR=/workdir/physalia-lcwgs/day_1/ # Note that no spaces are allowed!

```

<br>

##### Write the paths to required programs as variables

When running these scripts on the Physalia server, run the following:

```{bash eval=F}

FASTQC=fastqc
TRIMMOMATIC=trimmomatic
PICARD=/home/ubuntu/Software/picard-2.23.8/picard.jar
SAMTOOLS=samtools
BOWTIEBUILD=bowtie2-build
BOWTIE=bowtie2
BAMUTIL=
JAVA=/home/ubuntu/miniconda3/pkgs/java-jdk-8.0.92-1/bin/java

```

<br>


## Data processing pipeline

Now let's get started processing the data!

<br>

#### Examine the raw fastq files

##### fastq file structure

A FASTQ file normally contains four lines per sequence.

  * Line 1 contains the sequence identifier, with information on the sequencing run and the cluster. The exact content of this line varies depending on how fastq files are generated from the sequencer.
  * Line 2 is the raw sequence. 
  * Line 3 often consists of a single `+` symbol. 
  * Line 4 encodes the quality of each base in the sequence in Line 2 (i.e. the probability of sequencing error in log scale). For most current sequencers, these base qualities are encoded in the [Phred33 format](https://drive5.com/usearch/manual/quality_score.html), but always check to make sure how your quality scores are encoded. 

Now read the code below, guess what it does, and run it on your own. Does it do what you expect it to do? Inspect the output and try to identify the group of four lines for each read. 

> Hint: make sure that you have changed the `BASEDIR` path to your own base directory.

<br>

```{bash eval=F}

SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to the sample list.
RAWFASTQSUFFIX1=_1.fastq.gz # Suffix to raw fastq files. Use forward reads with paired-end data.

for SAMPLE in `cat $SAMPLELIST`; do

  echo $SAMPLE
  zcat $BASEDIR'raw_fastq/'$SAMPLE$RAWFASTQSUFFIX1 | head -n 8
  echo ' '

done

```

<br>

##### Evaluate the overall data quality

With a new batch of data, it is always to good idea to start out by getting an overview of the data quality and look for any signs of quality issues. The [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) program provides a useful set of diagnostics, so we'll run it on each on our fastq files to check their quality.

<br>

```{bash eval=F}

SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files. It should be a subset of the the 1st column of the sample table.
RAWFASTQSUFFIX1=_1.fastq.gz # Suffix to raw fastq files. Use forward reads with paired-end data.
RAWFASTQSUFFIX2=_2.fastq.gz # Suffix to raw fastq files. Use reverse reads with paired-end data.

for SAMPLE in `cat $SAMPLELIST`; do

  $FASTQC $BASEDIR'raw_fastq/'$SAMPLE$RAWFASTQSUFFIX1 -o $BASEDIR'fastqc/'
  $FASTQC $BASEDIR'raw_fastq/'$SAMPLE$RAWFASTQSUFFIX2 -o $BASEDIR'fastqc/'
  
done

```


Grab the html output files and examine

If you can't get the file transfer to work, you can download our's from here.

Which samples do you think came from which library?

<br>

#### Adapter clipping

When the insert length of a library fragment is shorter than the read length, the sequencer will read into the adapter sequence (as shown below). This means that the end of the read will not be from our actual sample, but will be adapter sequence, which may lead to lower alignment performance and even biases in the result if not removed.  

![](https://www.ecseq.com/support/ngs/img/fragmentsize.png)
<br>

We saw in our FastQC report that we have substantial adapter content in some of our libraries, so will will need to clip that off. Here, we use [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) to clip the adapter sequence off the ends of reads where they appear. This step requires us to input the known adapter sequences that we used when preparing the libraries (`ADAPTERS`). In this exercise, the libraries were prepared using Illumina's Nextera adapters (sequences listed in NexteraPE_NT.fa). 

Look over the code below. The first block of text specifies which files we are using as input. Then we start looping over our samples. Within the loop, the first step is to extract the relevant sample data from our sample table and assign those as temporary variables. Then we have two `if statements` to call the program with slightly different parameters for paired-end and single-end data.

<br>

```{bash eval=F}

SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files. It should be a subset of the the 1st column of the sample table.
SAMPLETABLE=$BASEDIR/sample_lists/sample_table.tsv # Path to a sample table where the 1st column is the prefix of the raw fastq files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The combination of these three columns have to be unique. The 6th column should be data type, which is either pe or se. 
RAWFASTQDIR=$BASEDIR/raw_fastq/ # Path to raw fastq files. 
RAWFASTQSUFFIX1=_1.fastq.gz # Suffix to raw fastq files. Use forward reads with paired-end data.
RAWFASTQSUFFIX2=_2.fastq.gz # Suffix to raw fastq files. Use reverse reads with paired-end data. 
ADAPTERS=$BASEDIR/reference/NexteraPE_NT.fa # Path to a list of adapter/index sequences.

## Loop over each sample
for SAMPLEFILE in `cat $SAMPLELIST`; do
	
	## Extract relevant values from a table of sample, sequencing, and lane ID (here in columns 4, 3, 2, respectively) for each sequenced library
	SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4`
	SEQ_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 3`
	LANE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 2`
	SAMPLE_SEQ_ID=$SAMPLE_ID'_'$SEQ_ID'_'$LANE_ID  # When a sample has been sequenced in multiple lanes, we need to be able to identify the files from each run uniquely
	
	## Extract data type from the sample table
	DATATYPE=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 6`
	
	## The input and output path and file prefix
	RAWFASTQ_ID=$RAWFASTQDIR$SAMPLEFILE
	SAMPLEADAPT=$BASEDIR'adapter_clipped/'$SAMPLE_SEQ_ID
	
	## Adapter clip the reads with Trimmomatic
	# The options for ILLUMINACLIP are: ILLUMINACLIP:<fastaWithAdaptersEtc>:<seed mismatches>:<palindrome clip threshold>:<simple clip threshold>:<minAdapterLength>:<keepBothReads>
	# For definitions of these options, see http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf
	
	if [ $DATATYPE = pe ]; then
		$TRIMMOMATIC PE -threads 18 -phred33 $RAWFASTQ_ID$RAWFASTQSUFFIX1 $RAWFASTQ_ID$RAWFASTQSUFFIX2 $SAMPLEADAPT'_adapter_clipped_f_paired.fastq.gz' $SAMPLEADAPT'_adapter_clipped_f_unpaired.fastq.gz' $SAMPLEADAPT'_adapter_clipped_r_paired.fastq.gz' $SAMPLEADAPT'_adapter_clipped_r_unpaired.fastq.gz' 'ILLUMINACLIP:'$ADAPTERS':2:30:10:1:true'
	
	elif [ $DATATYPE = se ]; then
		$TRIMMOMATIC SE -threads 18 -phred33 $RAWFASTQ_ID$RAWFASTQSUFFIX1 $SAMPLEADAPT'_adapter_clipped_se.fastq.gz' 'ILLUMINACLIP:'$ADAPTERS':2:30:10'
	fi
	
done

```

<br>

Have a look at the output printed to the screen. Do you notice a difference in the amount of sequence removed from the different libraries?

The output from Trimmomatic only shows how many full reads get removed, not how much the reads within the file get truncated. We could have added an additional parameters to remove reads shorter than a certain length (e.g. 50bp) after adapter trimming, which probably would have resulted in some more reads getting dropped. 

But we can check how much sequence in terms of bp actually got removed from the fastq files by comparing base counts with a simple bash command.

```{bash}



```


<br>

#### OPTIONAL: Quality trimming
As we saw in our FastQC output, the base call quality score tends to drop off towards the ends of the reads. As we'll learn more about tomorrow, probabilistic analysis frameworks, like angsd and others based on genotype likelihoods, can take the basecall quality into account and that way give less weight to a basecall that is less certain.

However, as a conservative measure, we may want to just trim off the rest of the read if the quality score drops too low over multiple bases. We can do this with the `SLIDINGWINDOW` module in [Trimmomatic](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf). We won't have time to do that in this practical, but if you're interested, you can modify the Trimmomatic code above to also trim off low-quality bases.

<br>

#### Build reference index files 

There are lots of different programs developed for mapping short reads to a reference sequence. We will use the program [bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml). This program requires a set of reference index files to be able to perform the sequence alignment. So we will start by indexing our reference.

<br>

```{bash eval=F}

REFERENCE=$BASEDIR/reference/mme_physalia_testdata_chr24.fa   # This is a fasta file with the reference genome sequence we will map to 
REFBASENAME="${REFERENCE%.*}"
$SAMTOOLS faidx $REFERENCE
java -jar $PICARD CreateSequenceDictionary R=$REFERENCE O=$REFBASENAME'.dict'
$BOWTIEBUILD $REFERENCE $REFBASENAME

```

<br>
<br>

#### Map to the reference, sort, and quality filter

In this step, we align the short reads within each fastq file to the reference genome using `bowtie2`. The resulting alignment file, in `sam` format, will be converted to a binary format `bam` for more efficient storage. Each mapped read will have a mapping quality, which indicates how confident that mapper is that a read is mapped in the correct position. The [bowtie2 manual](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml) defines it as "a non-negative integer Q = -10 log10 p, where p is an estimate of the probability that the alignment does not correspond to the read's true point of origin." Accordingly, a mapping quality (or MAPQ) of 10 or less indicates that there is at least a 1 in 10 chance that the read truly originated elsewhere, and a MAPQ of 20 indicates at least a 1 in 100 chance.

Here, to only retain reads for which we are reasonably certain have been mapped in the correct place, we will filter out reads with a mapping quality lower than 20, and after that sort the filtered alignment file for easier computation in the next step. 

Look over the code and make sure you understand what it's doing, then copy and run it.

<br>

```{bash eval=F}

SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files. It should be a subset of the the 1st column of the sample table.
SAMPLETABLE=$BASEDIR/sample_lists/sample_table.tsv # Path to a sample table where the 1st column is the prefix of the raw fastq files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The combination of these three columns have to be unique. The 6th column should be data type, which is either pe or se. 
FASTQDIR=$BASEDIR/adapter_clipped/ # Path to the directory where fastq file are stored. 
FASTQSUFFIX1=_adapter_clipped_f_paired.fastq.gz # Suffix to fastq files. Use forward reads with paired-end data. 
FASTQSUFFIX2=_adapter_clipped_r_paired.fastq.gz # Suffix to fastq files. Use reverse reads with paired-end data. 
MAPPINGPRESET=very-sensitive # The pre-set option to use for mapping in bowtie2 (very-sensitive for end-to-end (global) mapping [typically used when we have a full genome reference], very-sensitive-local for partial read mapping that allows soft-clipping [typically used when mapping genomic reads to a transcriptome]
REFERENCE=$BASEDIR/reference/mme_physalia_testdata_chr24.fa # Path to reference fasta file and file name
REFNAME=mme_physalia_testdata_chr24 # Reference name to add to output files, e.g. gadMor2

## Loop over each sample
for SAMPLEFILE in `cat $SAMPLELIST`; do
	
	## Extract relevant values from a table of sample, sequencing, and lane ID (here in columns 4, 3, 2, respectively) for each sequenced library
	SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4`
	SEQ_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 3`
	LANE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 2`
	SAMPLE_SEQ_ID=$SAMPLE_ID'_'$SEQ_ID'_'$LANE_ID
	
	## Extract data type from the sample table
	DATATYPE=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 6`
	
	## The input and output path and file prefix
	SAMPLETOMAP=$FASTQDIR$SAMPLE_SEQ_ID
	SAMPLEBAM=$BASEDIR'bam/'$SAMPLE_SEQ_ID
	
	## Define platform unit (PU), which is the lane number
	PU=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 2`
	
	## Define reference base name
	REFBASENAME="${REFERENCE%.*}"
	
	## Map reads to the reference 
	
	# Map the paired-end reads
	if [ $DATATYPE = pe ]; then 
	# We ignore the reads that get orphaned during adapter clipping because that is typically a very small proportion of reads. If a large proportion of reads get orphaned (loose their mate so they become single-end), these can be mapped in a separate step and the resulting bam files merged with the paired-end mapped reads.
	$BOWTIE -q --phred33 --$MAPPINGPRESET -p 16 -I 0 -X 1500 --fr --rg-id $SAMPLE_SEQ_ID --rg SM:$SAMPLE_ID --rg LB:$SAMPLE_ID --rg PU:$PU --rg PL:ILLUMINA -x $REFBASENAME -1 $SAMPLETOMAP$FASTQSUFFIX1 -2 $SAMPLETOMAP$FASTQSUFFIX2 -S $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.sam'
	
	# Map the single-end reads
	elif [ $DATATYPE = se ]; then
	$BOWTIE -q --phred33 --$MAPPINGPRESET -p 16 --rg-id $SAMPLE_SEQ_ID --rg SM:$SAMPLE_ID --rg LB:$SAMPLE_ID --rg PU:$PU --rg PL:ILLUMINA -x $REFBASENAME -U $SAMPLETOMAP$FASTQSUFFIX1 -S $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.sam'
	
	fi
	
	## Convert to bam file for storage
	$SAMTOOLS view -bS -F 4 $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.sam' > $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.bam'
	rm $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.sam'
	
	## Filter the mapped reads
	# Filter bam files to remove poorly mapped reads (non-unique mappings and mappings with a quality score < 20) -- do we want the quality score filter??
	$SAMTOOLS view -h -q 20 $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.bam' | $SAMTOOLS view -buS - | $SAMTOOLS sort -o $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'_minq20_sorted.bam'
	
done
```

<br>
<br>

#### Examine the bam files

SAM stands for Sequence Alignment/Map format. BAM is the binary format for sam files (which takes up much less space). It is a TAB-delimited text format consisting of a header section, which is optional, and an alignment section. If present, the header must be prior to the alignments. Header lines start with ‘@’, while alignment lines do not. Each alignment line has 11 mandatory fields for essential alignment information such as mapping position, and variable number of optional fields for flexible or aligner specific information. BAM is the binary version of the SAM format. 

See the full documentation of the sam file format [here](https://samtools.github.io/hts-specs/SAMv1.pdf) or a quick overview of the column descriptors [here](https://en.wikipedia.org/wiki/SAM_(file_format))

Note that we have converted our sam files to bam files. That's useful for saving disk space, but because bam files are binary, they are not human readable. However, we can use the `view` utility in the program [samtools](http://www.htslib.org/doc/samtools.html) to convert the content back to human readable output to we can examine our alignments.

As an example, let's look at the output for `985_lane1`. The following command can we used to inspect the first eight lines the sorted bam file for this sample.

`$SAMTOOLS view 985_1_1_pe_bt2_mme_physalia_testdata_chr24_minq20_sorted.bam | head -n 8`

<br>

Take a few minutes to look at the output and look at the [column descriptors](https://en.wikipedia.org/wiki/SAM_(file_format)) to understand its content. 

<br>

**OPTIONAL exercise:** Write a loop on your own to print the first three alignments of all the sorted bam files that you generated in the last step. You can use the general template code below as a starting point.

```{bash, eval = FALSE}

$SAMTOOLS view $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'_minq20_sorted.bam' | head -n 3

```

<br>
<br>

#### Merge samples that were sequenced multiple times

We have now mapped the two separate sets of fastq files for each sample (the separate sets generated in independent sequencing runs), so we also have two bam files for each sample. If the two sequencing runs were performed with different aliquots of the same library, we will need to merge the bam files before we remove duplicate sequences because the two sequencing lanes would have been sequencing the same set of molecules (so each may contain duplicate fragments also sequenced in the other). For most downstream analysis, it's also a lot more convenient to only have a single bam file per individual.

<br>

As part of the the merging and for downstream steps, we need a new sample table. This is because prior to merging, each row represent a pair of fastq files (forward and reverse, or a single fastq file if we're using single-end reads) and samples sequenced in multiple runs, would appear in multiple lines. After merging, we need each unique sample to only occupy a single row. The merged sample table also has a slightly different formatting. Since we'll also be working on bam files rather than fastq files downstream from this point, we need to list bam IDs rather than fastq IDs in the sample lists that specify which samples we want to loop over in a particular pipeline step. In the interest of time, we provide these merged sample table and lists in the `sample_lists` directory, but we also provide R-code that generates them from the your fastq-level sample table and list, so you don't need to do this manually if you're working with your own samples.   

<br>


<details>
  <summary>Click here to view the R code</summary>

##### New merged sample table and bam lists

```{r eval=F}

library(tidyverse) #install.packages("tidyverse) is you don't have it already

## Define base directory and reference name
basedir <- "/workdir/physalia-lcwgs/day_1/"
refname <- "mme_physalia_testdata_chr24"

## Create a merged table by keeping only one row for each unique sample
# seq_id, lane_number, and data_type are all replaced with "merged" for duplicated samples
sample_table <- read_tsv(paste0("../sample_lists/sample_table.tsv"))
sample_table_merged <- sample_table  %>%
  group_by(sample_id) %>%
  summarise(population = unique(population), seq_id = ifelse(n() == 1, seq_id, "merged"), 
            lane_number = ifelse(length(unique(lane_number))==1,unique(lane_number), "merged"),
            data_type = paste0(unique(data_type), collapse = "")) %>%
  mutate(sample_seq_id = paste(sample_id, seq_id, lane_number, data_type, sep = "_")) %>%
  select(sample_seq_id, lane_number, seq_id, sample_id, population, data_type)

## Write the merged table
write_tsv(sample_table_merged, paste0(basedir, "sample_lists/sample_table_merged.tsv"))

## Create bam lists as inputs for future steps
bam_list_merged <- paste0(basedir, "bam/", sample_table_merged$sample_seq_id, "_bt2_", refname, "_minq20_sorted.bam")

bam_list_dedup_overlapclipped <- transmute(sample_table_merged, suffix=ifelse(data_type=="se", paste0("_bt2_", refname, "_minq20_sorted_dedup.bam"), paste0("_bt2_", refname, "_minq20_sorted_dedup_overlapclipped.bam"))) %>%
  .$suffix %>%
  paste0(basedir, "bam/", sample_table_merged$sample_seq_id, .)

bam_list_realigned <- transmute(sample_table_merged, suffix=ifelse(data_type=="se", paste0("_bt2_", refname, "_minq20_sorted_dedup_realigned.bam"), paste0("_bt2_", refname, "_minq20_sorted_dedup_overlapclipped_realigned.bam"))) %>%
  .$suffix %>%
  paste0(basedir, "bam/", sample_table_merged$sample_seq_id, .)
write_lines(bam_list_merged, paste0(basedir, "sample_lists/bam_list_merged.txt"))
write_lines(bam_list_dedup_overlapclipped, paste0(basedir, "sample_lists/bam_list_dedup_overlapclipped.txt"))
write_lines(bam_list_realigned, paste0(basedir, "sample_lists/bam_list_realigned.txt"))

```
</details>

<br>
<br>

##### Create merging script (whole genome)

We will merge the two bam files for each individual with [samtools merge](http://www.htslib.org/doc/samtools-merge.html) with the following parameters

```{bash, eval = FALSE}

@SAMTOOLS merge merged.bam input1.bam input2.bam   # We replace the merged.bam with the name we want to give the output bam and the two input names with the names of the bam files we want to merge.

```

In this case, we will not use a for loop to iterate over our samples. Instead we will run a shell script that has a line that call `samtools merge` for each sample. You can find the script `merge_bams.sh` in the `scripts` folder. Have a look at it.

With just three samples, we could quickly copy and edit these three lines to construct this shell script manually. But if we had hundreds of samples, that approach would become error-prone. We've this provided an R-script below that will generate the merging script if you need to do this for your own samples. 

<details>
  <summary>Click here to view the R code</summary>
  
```{r eval=F}
## Find all duplicated samples
library(tidyverse)

basedir <- "/workdir/physalia-lcwgs/day_1/"

sample_table <- read_tsv(paste0(basedir, "sample_lists/sample_table.tsv"))
sample_table_merged <- read_tsv(paste0(basedir, "sample_lists/sample_table_merged.tsv"))


duplicated_samples <- (sample_table$sample_id)[duplicated(sample_table$sample_id)] %>% unique()
duplicated_samples_seq_ids <- sample_table_merged[match(duplicated_samples,sample_table_merged$sample_id),] %>%
  .$sample_seq_id

merging_script<-NULL


## Loop through all duplicated samples 
for (i in 1:length(duplicated_samples)){
  duplicated_sample <- duplicated_samples[i]
  duplicated_samples_seq_id <- duplicated_samples_seq_ids[i]
  ## Extract the bam file names from the unmerged sample table
  input <- filter(sample_table, sample_id==duplicated_sample) %>%
    mutate(unmerged_bam=paste(sample_id, seq_id, lane_number, data_type, "bt2", refname, "minq20_sorted.bam", sep = "_")) %>% 
    # Note that sample_id is used in here instead of sample_id, since the unmerged bam file still uses sample_id as part of its name, not the corrected one.
    .$unmerged_bam %>%
    paste0(basedir, "bam/", .) %>%
    paste(collapse = " ")
  
  ## Paste together the command line
  merging_script[i] <- paste0("samtools merge ", basedir, "bam/", duplicated_samples_seq_id, "_bt2_", refname, "_minq20_sorted.bam ", input)
}

## Write the script
write_lines(merging_script, paste0(basedir, "scripts/merge_bam.sh"))
```

</details>

<br>
<br>

##### Run the merging script
To execute the merging, run the bash script with the following command:

```{bash eval=F}

chmod u+x $BASEDIR/scripts/merge_bam.sh
bash $BASEDIR/scripts/merge_bam.sh 

```

<br>

Check that your merged bam files get generated. As a sanity check, we can compare the number of lines in our merged and set of unmerged bam files for each sample with [samtools view](http://www.htslib.org/doc/samtools-view.html) and the command `samtools view in.bam | wc -l`

Run on our server, for the merged file for sample 985, the command would like like

```{bash}

@SAMTOOLS view 985_merged_pe_bt2_mme_physalia_testdata_chr24_minq20_sorted.bam | wc -l

```

Check the line count in the bam files for the two individual fastqs and see if the numbers add up.

**OPTIONAL exercise**: You could write a for loop that will extract the line count for each file.

<br>
<br>

**Discussion question:** This merging procedure required some extra effort. Why didn't we just merge the fastq files for each individual before mapping?

<br>

##### Answer

<details>
  <summary>Click here</summary>
  
Because there may be particular issues associated each sequencing lane (in particular the base call quality score calibration may vary), so for some downstream analysis, we need to keep track of what data were sequenced in what lane. By mapping the fastq files separately, we were able to add read platform unit `PU` information to the read group tag in the bam file while mapping with `bowtie2` (see the script). This way we can continue to keep track of which read came from which lane, and could even filter our bam file based on this later on, if we ended up wanting to compare data from different lanes for troubleshooting or other reasons. 

</details>

<br>
<br>

#### Deduplicate (all samples) and clip overlapping read pairs (paired-end reads only)

Here, we remove the PCR duplicates and trim the overlapping part of each read pair in pair-end data. It is important to deduplicate after merging, because PCR duplicates for the same sample may exist in different lanes. 

![](https://i.stack.imgur.com/7dkOV.png)

```{bash eval=F}

BAMLIST=$BASEDIR/sample_lists/bam_list_merged.txt # Path to a list of merged bam files.
SAMPLETABLE=$BASEDIR/sample_lists/sample_table_merged.tsv # Path to a sample table where the 1st column is the prefix of the MERGED bam files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The 5th column is population name and 6th column is the data type.
REFNAME=mme_physalia_testdata_chr24 # Reference name to add to output files

## Loop over each sample
for SAMPLEBAM in `cat $BAMLIST`; do
	
	## Extract the file name prefix for this sample
	SAMPLEPREFIX=`echo $SAMPLEBAM | sed 's/_bt2_.*//' | sed -e 's#.*/bam/\(\)#\1#'`
	
	## Remove duplicates and print dupstat file
	# We used to be able to just specify picard.jar on the CBSU server, but now we need to specify the path and version
	java -Xmx60g -jar $PICARD MarkDuplicates I=$SAMPLEBAM O=$BASEDIR'bam/'$SAMPLEPREFIX'_bt2_'$REFNAME'_minq20_sorted_dedup.bam' M=$BASEDIR'bam/'$SAMPLEPREFIX'_bt2_'$REFNAME'_minq20_sorted_dupstat.txt' VALIDATION_STRINGENCY=SILENT REMOVE_DUPLICATES=true
	
	## Extract data type from the merged sample table
	DATATYPE=`grep -P "${SAMPLEPREFIX}\t" $SAMPLETABLE | cut -f 6`
	
	if [ $DATATYPE != se ]; then
		## Clip overlapping paired end reads (only necessary for paired-end data)
		$BAMUTIL clipOverlap --in $BASEDIR'bam/'$SAMPLEPREFIX'_bt2_'$REFNAME'_minq20_sorted_dedup.bam' --out $BASEDIR'bam/'$SAMPLEPREFIX'_bt2_'$REFNAME'_minq20_sorted_dedup_overlapclipped.bam' --stats
	fi
	
done
```

<br>
<br>

#### Indel realignment (optional)

Unlike other variant detector programs like the [GATK Haplotype Caller](https://gatk.broadinstitute.org/hc/en-us/articles/360037225632-HaplotypeCaller) or [Freebayes](https://github.com/ekg/freebayes), [angsd](http://www.popgen.dk/angsd/index.php/ANGSD) does not realign reads during its analysis. Because it can be difficult to distinguish indels from SNPs at the end of reads if each alignment is considered separately, indels may interfere with genotype likelihood estimation. We there recommend running your bam files through a program that realigns reads around indels prior to running `angsd`. The [GATK IndelRealigner](https://github.com/broadinstitute/gatk-docs/blob/master/gatk3-tutorials/(howto)_Perform_local_realignment_around_indels.md) takes all the aligned sequences from all samples in to account to validate the indels discovered from the mapping process and then realigns each read locally. We don't have time to run it today, but the code is provided here if you want to run it on your own.


<details>
  <summary>Click here to see the GATK IndelRealigner code</summary>

```{bash eval=F}
## Use an older version of Java
#export JAVA_HOME=/usr/local/jdk1.8.0_121
#export PATH=$JAVA_HOME/bin:$PATH

cp $BASEDIR/sample_lists/bam_list_dedup_overlapclipped.txt $BASEDIR/sample_lists/bam_list_dedup_overlapclipped.list

BAMLIST=$BASEDIR/sample_lists/bam_list_dedup_overlapclipped.list # Path to a list of merged, deduplicated, and overlap clipped bam files. Full paths should be included. This file has to have a suffix of ".list"
REFERENCE=$BASEDIR/reference/mme_physalia_testdata_chr24.fa # Path to reference fasta file and file name
REFNAME=mme_physalia_testdata_chr24 # Reference name to add to output files

## Loop over each sample
cd $BASEDIR/bam/
for SAMPLEBAM in `cat $BAMLIST`; do

if [ -e $SAMPLEBAM'.bai' ]; then
	echo "the file already exists"
else
	## Index bam files
	$SAMTOOLS index $SAMPLEBAM
fi

done

## Realign around in-dels
# This is done across all samples at once

## Create list of potential in-dels
$JAVA -Xmx40g -jar $GATK \
-T RealignerTargetCreator \
-R $REFERENCE \
-I $BAMLIST \
-o $BASEDIR'bam/all_samples_for_indel_realigner.intervals' \
-drf BadMate

## Run the indel realigner tool
$JAVA -Xmx40g -jar $GATK \
-T IndelRealigner \
-R $REFERENCE \
-I $BAMLIST \
-targetIntervals $BASEDIR'bam/all_samples_for_indel_realigner.intervals' \
--consensusDeterminationModel USE_READS  \
--nWayOut _realigned.bam
```

</details>

<br>
<br>

## Estimate read depth




#### Read count

##### Count fastq files

```{bash eval=F}
SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files. It should be a subset of the the 1st column of the sample table.
SAMPLETABLE=$BASEDIR/sample_lists/sample_table.tsv # Path to a sample table where the 1st column is the prefix of the raw fastq files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The combination of these three columns have to be unique. The 6th column should be data type, which is either pe or se. 
RAWFASTQDIR=$BASEDIR/raw_fastq/ # Path to raw fastq files. 
SEQUENCER=@HWI # Sequencer name that appears in the beginning of the first line in a fastq file. 
QUALFILTERED=false # Whether the sample has gone through quality filtering. true or false
OUT=$BASEDIR/sample_lists/fastq_count.tsv
# Create headers for the output
if $QUALFILTERED; then
	printf 'sample_seq_id\traw_reads\traw_bases\tadapter_clipped_bases\tqual_filtered_bases\n' > $OUT
else
	printf 'sample_seq_id\traw_reads\traw_bases\tadapter_clipped_bases\n' > $OUT
fi
# Loop over each sample in the sample table
for SAMPLEFILE in `cat $SAMPLELIST`; do
  RAWFASTQFILES=$RAWFASTQDIR$SAMPLEFILE'*.gz'  # The input path and file prefix
  
  # Count the number of reads in raw fastq files. We only need to count the forward reads, since the reverse will contain exactly   the same number of reads. fastq files contain 4 lines per read, so the number of total reads will be half of this line number. 
  RAWREADS=`zcat $RAWFASTQFILES | wc -l`
  
  # Count the number of bases in raw fastq files. We only need to count the forward reads, since the reverse will contain exactly   the same number of bases. The total number of reads will be twice this count. 
  RAWBASES=`zcat $RAWFASTQFILES | grep -A 1 -E "^$SEQUENCER" | grep "^[ACGTN]" | tr -d "\n" | wc -m` 
  
  # Extract relevant values from a table of sample, sequencing, and lane ID (here in columns 4, 3, 2, respectively) for each sequenced library
  SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4`
  SEQ_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 3`
  LANE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 2`
  SAMPLE_SEQ_ID=$SAMPLE_ID'_'$SEQ_ID'_'$LANE_ID
  
  # Find all adapter clipped fastq files corresponding to this sample and store them in the object ADAPTERFILES.
  ADAPTERFILES=$BASEDIR'adapter_clipped/'$SAMPLE_SEQ_ID'*.gz'
  
  # Count all bases in adapter clipped files. 
  ADPTERCLIPBASES=`zcat $ADAPTERFILES | grep -A 1 -E "^$SEQUENCER" | grep "^[ACGTN]" | tr -d "\n" | wc -m`
  
  # If reads are quality filtered, count quality filtered files.
  if $QUALFILTERED; then
    # Find all quality trimmed fastq files corresponding to this sample and store them in the object QUALFILES.
    QUALFILES=$BASEDIR'qual_filtered/'$SAMPLE_SEQ_ID'*.gz'
    # Count bases in quality trimmed files.
    QUALFILTPBASES=`zcat $QUALFILES | grep -A 1 -E "^$SEQUENCER" | grep "^[ACGTN]" | tr -d "\n" | wc -m`
    # Write the counts in appropriate order.
    printf "%s\t%s\t%s\t%s\t%s\n" $SAMPLE_SEQ_ID $((RAWREADS/4)) $RAWBASES $ADPTERCLIPBASES $QUALFILTPBASES >> $OUT
    # When reads are not quality filtered, directly write the output
  
  else
    # Write the counts in appropriate order.
    printf "%s\t%s\t%s\t%s\n" $SAMPLE_SEQ_ID $((RAWREADS/4)) $RAWBASES $ADPTERCLIPBASES >> $OUT
  fi
done
```

##### Count unmerged bam files

```{bash eval=F}
SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files. It should be a subset of the the 1st column of the sample table.
SAMPLETABLE=$BASEDIR/sample_lists/sample_table.tsv # Path to a sample table where the 1st column is the prefix of the raw fastq files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The combination of these three columns have to be unique. The 6th column should be data type, which is either pe or se. 
REFNAME=mme_physalia_testdata_chr24 # Reference name to add to output files
OUT=$BASEDIR/sample_lists/bam_count_unmerged.tsv
printf 'sample_seq_id\tmapped_bases\tqual_filtered_mapped_bases\n' > $OUT
for SAMPLEFILE in `cat $SAMPLELIST`; do
	
	# Extract relevant values from a table of sample, sequencing, and lane ID (here in columns 4, 3, 2, respectively) for each sequenced library
	SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4`
	SEQ_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 3`
	LANE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 2`
	SAMPLE_SEQ_ID=$SAMPLE_ID'_'$SEQ_ID'_'$LANE_ID
	
	## Extract data type from the sample table
	DATATYPE=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 6`
	
	## Count raw mapped bases
	RAWBAMFILE=$BASEDIR'bam/'$SAMPLE_SEQ_ID'_'$DATATYPE'_bt2_'$REFNAME'.bam'
	MAPPEDBASES=`$SAMTOOLS stats $RAWBAMFILE | grep ^SN | cut -f 2- | grep "^bases mapped (cigar)" | cut -f 2`
	
	## Count quality filtered mapped bases
	QUALFILTBAMFILE=$BASEDIR'bam/'$SAMPLE_SEQ_ID'_'$DATATYPE'_bt2_'$REFNAME'_minq20_sorted.bam'
	QUAFILTBASES=`$SAMTOOLS stats $QUALFILTBAMFILE | grep ^SN | cut -f 2- | grep "^bases mapped (cigar)" | cut -f 2`
	
	printf "%s\t%s\t%s\n" $SAMPLE_SEQ_ID $MAPPEDBASES $QUAFILTBASES >> $OUT
	
done
```

##### Count merged bam files

```{bash eval=F}
BAMLIST=$BASEDIR/sample_lists/bam_list_merged.txt # Path to a list of merged bam files.
SAMPLETABLE=$BASEDIR/sample_lists/sample_table_merged.tsv # Path to a sample table where the 1st column is the prefix of the MERGED bam files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The 5th column is population name and 6th column is the data type.
REFNAME=mme_physalia_testdata_chr24 # Reference name to add to output files
OUT=$BASEDIR/sample_lists/bam_count_merged.tsv
printf 'sample_seq_id\tdedup_mapped_bases\tavg_fragment_size\toverlap_clipped_bases\n' > $OUT
for SAMPLEBAM in `cat $BAMLIST`; do
  
  ## Extract the file name prefix for this sample
  SAMPLEPREFIX=`echo $SAMPLEBAM | sed 's/_bt2_.*//' | sed -e 's#.*/bam/\(\)#\1#'`
  
  ## Count deduplicated bases
  DEDUPFILE=$BASEDIR'bam/'$SAMPLEPREFIX'_bt2_'$REFNAME'_minq20_sorted_dedup.bam'
  DEDUPMAPPEDBASES=`$SAMTOOLS stats $DEDUPFILE | grep ^SN | cut -f 2- | grep "^bases mapped (cigar)" | cut -f 2`
  
  ## Extract data type from the merged sample table
  DATATYPE=`grep -P "${SAMPLEPREFIX}\t" $SAMPLETABLE | cut -f 6`
  
  if [ $DATATYPE != se ]; then
    ## Calculate average fragment length for paired end reads
    AVGFRAG=`$SAMTOOLS view $DEDUPFILE | grep YT:Z:CP | awk '{sum+=sqrt($9^2)} END {printf "%f", sum/NR}'`
  if [ "$AVGFRAG" == '' ]; then AVGFRAG=0 ; fi
    
    ## Count overlap clipped bam files for paired end reads 
    CLIPOVERLAPFILE=$BASEDIR'bam/'$SAMPLEPREFIX'_bt2_'$REFNAME'_minq20_sorted_dedup_overlapclipped.bam'
    CLIPOVERLAPBASES=`$SAMTOOLS stats $CLIPOVERLAPFILE | grep ^SN | cut -f 2- | grep "^bases mapped (cigar)" | cut -f 2`
    
  else
    AVGFRAG=NA
    CLIPOVERLAPBASES=NA
  fi
printf "%s\t%s\t%s\t%s\n" $SAMPLEPREFIX $DEDUPMAPPEDBASES $AVGFRAG $CLIPOVERLAPBASES >> $OUT
done
```

#### Summarize counting result

```{r eval=F, message=F, warning=F, fig.width=12, fig.height=4}
basedir="/workdir/physalia-lcwgs/day_1/"
library(tidyverse)
library(cowplot)
library(knitr)
fastq_count <- read_tsv(paste0(basedir, "/sample_lists/fastq_count.tsv")) %>% 
  mutate(sample_id=str_sub(sample_seq_id, 1, 3)) %>% 
  dplyr::select(-sample_seq_id) 
fastq_count_sum <- fastq_count %>%
  group_by(sample_id) %>%
  summarise(raw_bases=sum(raw_bases), adapter_clipped_bases=sum(adapter_clipped_bases))
bam_count_unmerged <- read_tsv(paste0(basedir, "/sample_lists/bam_count_unmerged.tsv")) %>% 
  mutate(sample_id=str_sub(sample_seq_id, 1, 3)) %>% 
  dplyr::select(-sample_seq_id)
bam_count_unmerged_sum <- bam_count_unmerged %>%
  group_by(sample_id) %>%
  summarise(mapped_bases=sum(mapped_bases), qual_filtered_mapped_bases=sum(qual_filtered_mapped_bases))
bam_count_merged <- read_tsv(paste0(basedir, "/sample_lists/bam_count_merged.tsv")) %>% 
  mutate(sample_id=str_sub(sample_seq_id, 1, 3)) %>% 
  dplyr::select(-sample_seq_id)
count_final <- left_join(fastq_count_sum, bam_count_unmerged_sum, by="sample_id") %>%
  left_join(bam_count_merged, by="sample_id") %>%
  relocate(sample_id) %>%
  relocate(-avg_fragment_size)
count_final %>% kable()
count_final %>% 
  pivot_longer(cols = 2:7, names_to = "step", values_to = "base_count") %>%
  arrange(sample_id, desc(base_count)) %>%
  mutate(step=fct_reorder(step, base_count, mean, .desc=T)) %>%
  ggplot(aes(x=sample_id, y=base_count/2/10^6, fill=step)) +
  geom_col(position="identity", col="black") +
  ylab("average coverage") +
  coord_flip() +
  theme_cowplot()
```

